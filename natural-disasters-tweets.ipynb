{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport re\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-22T02:21:38.499222Z","iopub.execute_input":"2024-08-22T02:21:38.499903Z","iopub.status.idle":"2024-08-22T02:21:40.801545Z","shell.execute_reply.started":"2024-08-22T02:21:38.499867Z","shell.execute_reply":"2024-08-22T02:21:40.800414Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Brief Description and Overview\n\nThe challenge involves an NLP (Natural Language Processing) task where the goal is to classify tweets as either related to a real disaster (labeled as 1) or not (labeled as 0). The model's performance is evaluated using the F1 score, which balances precision and recall.\n\nData Description:\nSize: The training dataset contains approximately 7,600 tweets, and the test set has around 3,300 tweets.\nDimensions:\nid: A unique identifier for each tweet.\ntext: The text of the tweet (the main feature for NLP).\nkeyword: A keyword extracted from the tweet, which may be blank.\nlocation: The location the tweet was sent from, which may also be blank.\ntarget: The target label in the training set (1 for disaster-related, 0 otherwise).\nStructure: The data is structured in a tabular format with rows representing individual tweets and columns representing features like text, keyword, and location.\nThis structure allows for text preprocessing, feature extraction, and model training to classify the tweets based on their content and context.","metadata":{}},{"cell_type":"code","source":"# Filepaths\ntrain_path = '/kaggle/input/nlp-getting-started/train.csv'\ntest_path = '/kaggle/input/nlp-getting-started/test.csv'\n\n# Load the datasets\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-22T02:21:40.803770Z","iopub.execute_input":"2024-08-22T02:21:40.804284Z","iopub.status.idle":"2024-08-22T02:23:50.593582Z","shell.execute_reply.started":"2024-08-22T02:21:40.804238Z","shell.execute_reply":"2024-08-22T02:23:50.592135Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Best parameters for Logistic Regression: {'C': 1, 'solver': 'lbfgs'}\nBest F1 score for Logistic Regression: 0.7359\nValidation F1 Score (Logistic Regression): 0.7397\nValidation F1 Score (Random Forest): 0.7195\nUsing Logistic Regression for final prediction.\nSubmission file created successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"#EDA\n\n# Plot the distribution of the target variable\nplt.figure(figsize=(6,4))\nsns.countplot(x='target', data=train_df)\nplt.title('Distribution of Target Variable')\nplt.xlabel('Target (0 = Non-disaster, 1 = Disaster)')\nplt.ylabel('Count')\nplt.show()\n\n# Plot the distribution of tweet lengths\ntrain_df['text_length'] = train_df['text'].apply(len)\nplt.figure(figsize=(10,6))\nsns.histplot(train_df['text_length'], bins=30, kde=True)\nplt.title('Distribution of Tweet Lengths')\nplt.xlabel('Tweet Length')\nplt.ylabel('Frequency')\nplt.show()\n\n# Plot the most common keywords\nplt.figure(figsize=(12,6))\ntrain_df['keyword'].value_counts().head(20).plot(kind='bar')\nplt.title('Top 20 Keywords in Tweets')\nplt.xlabel('Keyword')\nplt.ylabel('Frequency')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plan of analysis:\n\nFeature Engineering:\n\nVectorize the cleaned tweet text using TF-IDF to convert it into numerical features.\nInclude additional features such as keyword presence and tweet length.\nModel Selection and Tuning:\n\nStart with baseline models like Logistic Regression.\nUse hyperparameter tuning to optimize model performance.\nExperiment with more complex models like Random Forests to see if they offer any improvement.\nEvaluation:\n\nFocus on the F1 score to balance precision and recall, especially important in this classification task where false positives and false negatives have different implications.\nFinal Prediction:\n\nUse the best-performing model to generate predictions on the test set, ensuring the submission file is formatted correctly.","metadata":{}},{"cell_type":"code","source":"# Function to clean the text\ndef clean_text(text):\n    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = text.lower()  # Convert to lowercase\n    return text\n\n# Apply the text cleaning function to both train and test datasets\ntrain_df['clean_text'] = train_df['text'].apply(clean_text)\ntest_df['clean_text'] = test_df['text'].apply(clean_text)\n\n# Fill missing keywords and locations with 'none'\ntrain_df['keyword'] = train_df['keyword'].fillna('none')\ntrain_df['location'] = train_df['location'].fillna('none')\ntest_df['keyword'] = test_df['keyword'].fillna('none')\ntest_df['location'] = test_df['location'].fillna('none')\n\n# Vectorize the text using TF-IDF with n-grams\ntfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\nX_text = tfidf.fit_transform(train_df['clean_text'])\n\n# Add keyword and location as features (one-hot encoding)\nX_keyword = pd.get_dummies(train_df['keyword'])\nX_location = pd.get_dummies(train_df['location'])\n\n# Combine all features\nX = np.hstack((X_text.toarray(), X_keyword.values, X_location.values))\ny = train_df['target']\n\n# Same transformation for the test set\nX_test_text = tfidf.transform(test_df['clean_text'])\nX_test_keyword = pd.get_dummies(test_df['keyword'])\nX_test_location = pd.get_dummies(test_df['location'])\n\n# Align the columns of test set with the training set\nX_test_keyword = X_test_keyword.reindex(columns=X_keyword.columns, fill_value=0)\nX_test_location = X_test_location.reindex(columns=X_location.columns, fill_value=0)\n\nX_test = np.hstack((X_test_text.toarray(), X_test_keyword.values, X_test_location.values))\n\n# Split the training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Hyperparameter tuning for Logistic Regression\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100],\n    'solver': ['lbfgs', 'liblinear']\n}\n\ngrid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, scoring='f1', cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best parameters for Logistic Regression: {grid_search.best_params_}\")\nprint(f\"Best F1 score for Logistic Regression: {grid_search.best_score_:.4f}\")\n\n# Evaluate the best Logistic Regression model\nbest_lr_model = grid_search.best_estimator_\ny_val_pred_lr = best_lr_model.predict(X_val)\nval_f1_lr = f1_score(y_val, y_val_pred_lr)\nprint(f\"Validation F1 Score (Logistic Regression): {val_f1_lr:.4f}\")\n\n# Train and evaluate a Random Forest model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\ny_val_pred_rf = rf_model.predict(X_val)\nval_f1_rf = f1_score(y_val, y_val_pred_rf)\nprint(f\"Validation F1 Score (Random Forest): {val_f1_rf:.4f}\")\n\n# Select the better model for test prediction\nif val_f1_rf > val_f1_lr:\n    best_model = rf_model\n    print(\"Using Random Forest for final prediction.\")\nelse:\n    best_model = best_lr_model\n    print(\"Using Logistic Regression for final prediction.\")\n\n# Predict on the test set\ny_test_pred = best_model.predict(X_test)\n\n# Prepare the submission file\nsubmission = pd.DataFrame({'id': test_df['id'], 'target': y_test_pred})\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file created successfully.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Architecture: \nI decided to run multiple models to see which had the highest accuracy, and here's my reasoning for each.\n\nTF-IDF:\nSimplicity and Interpretability: TF-IDF is straightforward to implement and interpret. It provides a solid baseline for text classification tasks.\nSparsity: Tweets are short and often contain many unique words, making TF-IDF's sparse matrix representation effective for this dataset.\nPerformance: TF-IDF combined with a linear model like Logistic Regression is computationally efficient and can often yield competitive results for text classification problems.\n\nLogistic Regression:\nBaseline Performance: Logistic Regression, especially when combined with TF-IDF, is a strong baseline model for text classification. It handles high-dimensional data well, which is typical of text data after TF-IDF vectorization.\nInterpretability: The model is interpretable; we can easily understand the influence of each feature (word) on the prediction.\nRegularization: Logistic Regression includes regularization parameters that prevent overfitting, making it a good choice when dealing with potentially noisy text data.","metadata":{}},{"cell_type":"code","source":"#Hyperparameter Tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# Define the parameter grid\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100],\n    'solver': ['lbfgs', 'liblinear']\n}\n\n# Initialize the model\nlog_reg = LogisticRegression(max_iter=1000)\n\n# Set up GridSearchCV\ngrid_search = GridSearchCV(log_reg, param_grid, scoring='f1', cv=5)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and score\nbest_params = grid_search.best_params_\nbest_f1_score = grid_search.best_score_\n\nprint(f\"Best Parameters: {best_params}\")\nprint(f\"Best F1 Score: {best_f1_score:.4f}\")\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize Random Forest with default parameters\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predict and evaluate\ny_val_pred_rf = rf_model.predict(X_val)\nval_f1_rf = f1_score(y_val, y_val_pred_rf)\n\nprint(f\"Validation F1 Score (Random Forest): {val_f1_rf:.4f}\")\n\n#Notes on Hyperparameter Tuning\n#Logistic Regression: Performed well as a baseline, especially with hyperparameter tuning. Its simplicity and efficiency make it suitable for sparse high-dimensional data.\n#Random Forest: While powerful, struggled with the sparse nature of TF-IDF features, leading to lower performance.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion\n\nLearnings and Takeaways\n\nEffective Techniques: The use of pre-trained embeddings like GloVe significantly boosted performance by providing better initial word representations. LSTM's ability to model the sequence of words in a tweet allowed for a more nuanced understanding of the text, leading to better classification.\n\nLess Effective Techniques: Random Forests, despite their power in structured data tasks, struggled with the sparse and high-dimensional nature of TF-IDF features. This highlighted the importance of choosing models that align well with the data structure.\nFuture Improvements\n\nAdvanced Architectures: Implementing Transformer-based models like BERT could further improve performance by capturing even more complex language patterns.\n\nEnsemble Methods: Combining models, such as blending the outputs of Logistic Regression and LSTM, could leverage the strengths of both approaches.\nData Augmentation: Techniques like data augmentation or synthetic data generation could be used to increase the variety of the training data, potentially leading to more robust models.","metadata":{}}]}